{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 3e987287-fc08-4775-bb5d-1e5e82f35b2a\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\nWaiting for session 3e987287-fc08-4775-bb5d-1e5e82f35b2a to get into ready status...\nSession 3e987287-fc08-4775-bb5d-1e5e82f35b2a has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='db_sales_raw_catalog', table_name='sales_table_raw')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- name: string\n|-- address: string\n|-- city: string\n|-- country: string\n|-- region: string\n|-- productname: string\n|-- productcategory: string\n|-- productcategorydescription: string\n|-- productunitprice: string\n|-- quantityorderded: string\n|-- orderdate: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show(5)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+--------------+-------+--------------+--------------------+--------------------+--------------------------+--------------------+--------------------+--------------------+\n|                name|             address|          city|country|        region|         productname|     productcategory|productcategorydescription|    productunitprice|    quantityorderded|           orderdate|\n+--------------------+--------------------+--------------+-------+--------------+--------------------+--------------------+--------------------------+--------------------+--------------------+--------------------+\n|Zbyszek Piestrzen...|     ul. Filtrowa 68|      Warszawa| Poland|Eastern Europe|Gorgonzola Telino...|Dairy Products;Se...|      Cheeses;Seaweed a...|12.5;13.25;18;19;...|30;15;6;10;15;10;...|20121205;20121205...|\n|    Jonas Bergulfsen|Erling Skakkes ga...|       Stavern| Norway|   Scandinavia|Guarana Fantastic...|Beverages;Produce...|      Soft drinks, coff...|4.5;45.6;55;21.5;...|15;6;12;15;8;5;8;...|20121218;20121218...|\n|    Bernardo Batista|Rua da Panificado...|Rio de Janeiro| Brazil| South America|Sir Rodney's Scon...|Confections;Bever...|      Desserts, candies...|10;18;6;19.45;53;...|20;20;20;24;2;8;1...|20120719;20120719...|\n|      Karl Jablonski|305 - 14th Ave. S...|       Seattle|    USA| North America|Geitost;Mozzarell...|Dairy Products;Da...|      Cheeses;Cheeses;S...|2.5;34.8;22;40;19...|60;20;35;70;40;35...|20120731;20120731...|\n|    Pirkko Koskitalo|         Torikatu 38|          Oulu|Finland|   Scandinavia|Queso Manchego La...|Dairy Products;Se...|      Cheeses;Seaweed a...|38;19;46;21.5;23....|12;30;25;30;10;10...|20120726;20120801...|\n+--------------------+--------------------+--------------+-------+--------------+--------------------+--------------------+--------------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create Dataframes for all the tables in the structure same as normalized tables\n\n- Region\n    - [RegionID] Integer not null primary key\n    - [Region] Text not null\n- Country\n  - [CountryID] integer not null Primary key\n  - [Country] Text not null\n  - [RegionID] integer not null foreign key to Region table\n- Customer\n  - [CustomerID] integer not null Primary Key\n  - [FirstName] Text not null\n  - [LastName] Text not null\n  - [Address] Text not null\n  - [City] Text not null\n  - [CountryID] integer not null foreign key to Country table \n- ProductCateogry\n  - [ProductCategoryID] integer not null Primary Key\n  - [ProductCategory] Text not null\n  - [ProductCategoryDescription] Text not null\n- Product\n  - [ProductID] integer not null Primary key\n  - [ProductName] Text not null\n  - [ProductUnitPrice] Real not null\n  - [ProductCategoryID] integer not null foreign key to ProductCateogry table\n- OrderDetail\n  - [OrderID] integer not null Primary Key\n  - [CustomerID] integer not null foreign key to Customer table\n  - [ProductID] integer not null foreign key to Product table\n  - [OrderDate] integer not null \n  - [QuantityOrdered] integer not null",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df=df.dropDuplicates()\nrdd_df=df.rdd",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "region_rdd = (\n        rdd_df.map(lambda line: line[4])  # Select the 3rd column (indexing starts from 0)\n           .distinct()                             # Deduplicate the regions\n           .sortBy(lambda x: x)                    # Order the regions\n           .zipWithIndex()                         # Add index starting from 0\n           .map(lambda x: (int(x[1] + 1), x[0]))        # Increment index by 1 and create tuples (RegionID, Region)\n    )\nregion_df= region_rdd.toDF([\"RegionID\", \"Region\"])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "region_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+---------------+\n|RegionID|         Region|\n+--------+---------------+\n|       1|  British Isles|\n|       2|Central America|\n|       3| Eastern Europe|\n|       4|  North America|\n|       5|Northern Europe|\n|       6|    Scandinavia|\n|       7|  South America|\n|       8|Southern Europe|\n|       9| Western Europe|\n+--------+---------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Region Dataframe\n# from pyspark.sql.types import *\n# from pyspark.sql.functions import monotonically_increasing_id\n\n# schema = StructType([\n#     StructField(\"RegionID\", IntegerType(), True),\n#     StructField(\"Region\", StringType(), True)\n# ])\n# region_df=spark.createDataFrame([], schema)\n# region_df = (\n#     df.select(\"Region\")              # Select the 'Region' column\n#       .distinct()                    # Deduplicate the rows\n#       .orderBy(\"Region\")             # Order the DataFrame by 'Region'\n#       .withColumn(\"RegionID\", monotonically_increasing_id() + 1)  # Add 'RegionID' column starting from 1\n#       .select(\"RegionID\", \"Region\")  # Select only 'RegionID' and 'Region' columns\n# )\n# region_df.show(30)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Region to RegionID Dict\nregion_dict = region_df.select('Region', 'RegionID').rdd.collectAsMap()\nregion_dict",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'British Isles': 1, 'Central America': 2, 'Eastern Europe': 3, 'North America': 4, 'Northern Europe': 5, 'Scandinavia': 6, 'South America': 7, 'Southern Europe': 8, 'Western Europe': 9}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "country_rdd = (\n        rdd_df.map(lambda line:(line[3],line[4]))\n           .distinct()                            \n           .sortBy(lambda x: x[0])\n           .zipWithIndex()                        \n           .map(lambda x: (int(x[1]+1), x[0][0],int(region_dict.get(x[0][1], None))))\n)\ncountry_df= country_rdd.toDF([\"CountryID\", \"Country\",\"RegionID\"])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "country_df.show(25)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+-----------+--------+\n|CountryID|    Country|RegionID|\n+---------+-----------+--------+\n|        1|  Argentina|       7|\n|        2|    Austria|       9|\n|        3|    Belgium|       9|\n|        4|     Brazil|       7|\n|        5|     Canada|       4|\n|        6|    Denmark|       5|\n|        7|    Finland|       6|\n|        8|     France|       9|\n|        9|    Germany|       9|\n|       10|    Ireland|       1|\n|       11|      Italy|       8|\n|       12|     Mexico|       2|\n|       13|     Norway|       6|\n|       14|     Poland|       3|\n|       15|   Portugal|       8|\n|       16|      Spain|       8|\n|       17|     Sweden|       5|\n|       18|Switzerland|       9|\n|       19|         UK|       1|\n|       20|        USA|       4|\n|       21|  Venezuela|       7|\n+---------+-----------+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# # Country Dataframe\n# from pyspark.sql.functions import col, create_map, lit\n# from itertools import chain\n# mapping_expr = create_map([lit(x) for x in chain(*region_dict.items())]) \n\n# schema2 = StructType([\n#     StructField(\"CountryID\", IntegerType(), True),\n#     StructField(\"Country\", StringType(), True),\n#     StructField(\"RegionID\", IntegerType(), True)\n# ])\n# country_df=spark.createDataFrame([], schema2)\n# country_df = (\n#     df.select(\"Country\", \"Region\")\n#       .distinct()\n#       .orderBy(\"Country\")\n#       .withColumn(\"CountryID\", monotonically_increasing_id() + 1)\n#       .withColumn(\"RegionID\", mapping_expr[col(\"Region\")])\n#       .select(\"CountryID\", \"Country\",\"RegionID\")\n# )\n# country_df.show(25)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Country Dictionary\ncountry_dict = country_df.select('Country', 'CountryID').rdd.collectAsMap()\ncountry_dict",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'Argentina': 1, 'Austria': 2, 'Belgium': 3, 'Brazil': 4, 'Canada': 5, 'Denmark': 6, 'Finland': 7, 'France': 8, 'Germany': 9, 'Ireland': 10, 'Italy': 11, 'Mexico': 12, 'Norway': 13, 'Poland': 14, 'Portugal': 15, 'Spain': 16, 'Sweden': 17, 'Switzerland': 18, 'UK': 19, 'USA': 20, 'Venezuela': 21}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Customer Dataframe\ncustomer_rdd=(\nrdd_df.map(lambda line: (tuple(line[0].split(' ',1)),line[1],line[2],line[3]))\n    .distinct()\n    .sortBy(lambda x: (x[0][0],x[0][1]))\n    .zipWithIndex()\n    .map(lambda x: (int(x[1]+1),x[0][0][0],x[0][0][1],x[0][1],x[0][2],int(country_dict.get(x[0][3],None))))\n)\ncustomer_df= customer_rdd.toDF([\"CustomerID\",\"FirstName\",\"LastName\", \"Address\",\"City\",\"CountryID\"])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customer_df.show(100)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+---------------+--------------------+---------------+---------+\n|CustomerID| FirstName|       LastName|             Address|           City|CountryID|\n+----------+----------+---------------+--------------------+---------------+---------+\n|         1| Alejandra|         Camino|         Gran Via, 1|         Madrid|       16|\n|         2| Alexander|          Feuer|         Heerstr. 22|        Leipzig|        9|\n|         3|       Ana|       Trujillo|Avda. de la Const...|    Mexico D.F.|       12|\n|         4|   Anabela|      Domingues|Av. Ines de Castr...|      Sao Paulo|        4|\n|         5|     Andre|        Fonseca|     Av. Brasil, 442|       Campinas|        4|\n|         6|       Ann|          Devon|      35 King George|         London|       19|\n|         7|   Annette|         Roulet|1 rue Alsace-Lorr...|       Toulouse|        8|\n|         8|   Antonio|         Moreno|     Mataderos  2312|    Mexico D.F.|       12|\n|         9|      Aria|           Cruz|        Rua Oros, 92|      Sao Paulo|        4|\n|        10|       Art| Braunschweiger|        P.O. Box 555|         Lander|       20|\n|        11|  Bernardo|        Batista|Rua da Panificado...| Rio de Janeiro|        4|\n|        12|    Carine|        Schmitt|      54, rue Royale|         Nantes|        8|\n|        13|    Carlos|       Gonzalez|Carrera 52 con Av...|   Barquisimeto|       21|\n|        14|    Carlos|      Hernandez|Carrera 22 con Av...|  San Cristobal|       21|\n|        15| Catherine|          Dewey| Rue Joseph-Bens 532|      Bruxelles|        3|\n|        16| Christina|       Berglund|     Berguvsvagen  8|          Lulea|       17|\n|        17|    Daniel|         Tonini|67, avenue de l'E...|     Versailles|        8|\n|        18|     Diego|           Roel|  C/ Moralzarzal, 86|         Madrid|       16|\n|        19| Dominique|        Perrier|   25, rue Lauriston|          Paris|        8|\n|        20|   Eduardo|       Saavedra|Rambla de Catalun...|      Barcelona|       16|\n|        21| Elizabeth|          Brown|Berkeley Gardens ...|         London|       19|\n|        22| Elizabeth|        Lincoln|  23 Tsawassen Blvd.|      Tsawassen|        5|\n|        23|    Felipe|      Izquierdo|Ave. 5 de Mayo Po...|I. de Margarita|       21|\n|        24|      Fran|         Wilson|  89 Chiaroscuro Rd.|       Portland|       20|\n|        25| Francisco|          Chang|Sierras de Granad...|    Mexico D.F.|       12|\n|        26|Frederique|        Citeaux|    24, place Kleber|     Strasbourg|        8|\n|        27|     Georg|          Pipps|         Geislweg 14|       Salzburg|        2|\n|        28|  Giovanni|        Rovelli|Via Ludovico il M...|        Bergamo|       11|\n|        29| Guillermo|      Fernandez|Calle Dr. Jorge C...|    Mexico D.F.|       12|\n|        30|     Hanna|           Moos|      Forsterstr. 57|       Mannheim|        9|\n|        31|      Hari|          Kumar|     90 Wadhurst Rd.|         London|       19|\n|        32|     Helen|        Bennett|Garden House Crow...|          Cowes|       19|\n|        33| Helvetius|           Nagy|   722 DaVinci Blvd.|       Kirkland|       20|\n|        34| Henriette|      Pfalzheim|  Mehrheimerstr. 369|           Koln|        9|\n|        35|     Horst|          Kloss|     Taucherstrae 10|      Cunewalde|        9|\n|        36|    Howard|         Snyder|    2732 Baker Blvd.|         Eugene|       20|\n|        37|    Isabel|      de Castro|Estrada da saude ...|         Lisboa|       15|\n|        38|     Jaime|         Yorres| 87 Polk St. Suite 5|  San Francisco|       20|\n|        39|    Janete|        Limeira| Av. Copacabana, 267| Rio de Janeiro|        4|\n|        40|    Janine|        Labrune|67, rue des Cinqu...|         Nantes|        8|\n|        41|      Jean|      Fresniere|  43 rue St. Laurent|       Montreal|        5|\n|        42|      John|          Steel|12 Orchestra Terrace|    Walla Walla|       20|\n|        43|     Jonas|     Bergulfsen|Erling Skakkes ga...|        Stavern|       13|\n|        44|      Jose|      Pavarotti|     187 Suffolk Ln.|          Boise|       20|\n|        45|      Jose|   Pedro Freyre|       C/ Romero, 33|        Sevilla|       16|\n|        46|     Jytte|       Petersen|         Vinbltet 34|      Kobenhavn|        6|\n|        47|     Karin|        Josephs|       Luisenstr. 48|        Munster|        9|\n|        48|      Karl|      Jablonski|305 - 14th Ave. S...|        Seattle|       20|\n|        49|  Laurence|        Lebihan|12, rue des Bouchers|      Marseille|        8|\n|        50|      Lino|      Rodriguez|Jardim das rosas ...|         Lisboa|       15|\n|        51|       Liu|           Wong| 55 Grizzly Peak Rd.|          Butte|       20|\n|        52|       Liz|          Nixon|89 Jefferson Way ...|       Portland|       20|\n|        53|     Lucia|       Carvalho|Alameda dos Canar...|      Sao Paulo|        4|\n|        54|    Manuel|        Pereira|5 Ave. Los Palos ...|        Caracas|       21|\n|        55|     Maria|         Anders|       Obere Str. 57|         Berlin|        9|\n|        56|     Maria|        Larsson|        Akergatan 24|         Bracke|       17|\n|        57|     Marie|       Bertrand|265, boulevard Ch...|          Paris|        8|\n|        58|     Mario|         Pontes|     Rua do Paco, 67| Rio de Janeiro|        4|\n|        59|    Martin|         Sommer|      C/ Araquil, 67|         Madrid|       16|\n|        60|   Martine|          Rance|184, chaussee de ...|          Lille|        8|\n|        61|      Mary|        Saveley|  2, rue du Commerce|           Lyon|        8|\n|        62|     Matti|      Karttunen|       Keskuskatu 45|       Helsinki|        7|\n|        63|  Maurizio|         Moroni|Strada Provincial...|  Reggio Emilia|       11|\n|        64|   Michael|           Holz|   Grenzacherweg 237|         Geneve|       18|\n|        65|    Miguel|  Angel Paolino|    Avda. Azteca 123|    Mexico D.F.|       12|\n|        66|     Palle|          Ibsen|       Smagsloget 45|          Arhus|        6|\n|        67|     Paolo|        Accorti| Via Monte Bianco 34|         Torino|       11|\n|        68|   Pascale|       Cartrain|Boulevard Tirou, 255|      Charleroi|        3|\n|        69|  Patricia|        McKenna|    8 Johnstown Road|           Cork|       10|\n|        70|  Patricio|        Simpson|         Cerrito 333|   Buenos Aires|        1|\n|        71|      Paul|        Henriot|  59 rue de l'Abbaye|          Reims|        8|\n|        72|     Paula|        Parente|  Rua do Mercado, 12|        Resende|        4|\n|        73|     Paula|         Wilson|     2817 Milton Dr.|    Albuquerque|       20|\n|        74|     Pedro|         Afonso|Av. dos Lusiadas, 23|      Sao Paulo|        4|\n|        75|     Peter|        Franken|   Berliner Platz 43|        Munchen|        9|\n|        76|    Philip|         Cramer|       Maubelstr. 90|    Brandenburg|        9|\n|        77|    Pirkko|      Koskitalo|         Torikatu 38|           Oulu|        7|\n|        78|    Renate|        Messner|        Magazinweg 7| Frankfurt a.M.|        9|\n|        79|      Rene|       Phillips|     2743 Bering St.|      Anchorage|       20|\n|        80|      Rita|         Muller|   Adenauerallee 900|      Stuttgart|        9|\n|        81|    Roland|         Mendel|        Kirchgasse 6|           Graz|        2|\n|        82|    Sergio|      Gutierrez|Av. del Libertado...|   Buenos Aires|        1|\n|        83|     Simon|       Crowther|South House 300 Q...|         London|       19|\n|        84|      Sven|        Ottlieb|        Walserweg 21|         Aachen|        9|\n|        85|    Thomas|          Hardy|     120 Hanover Sq.|         London|       19|\n|        86|  Victoria|       Ashworth|   Fauntleroy Circus|         London|       19|\n|        87|      Yang|           Wang|        Hauptstr. 29|           Bern|       18|\n|        88|     Yoshi|        Latimer|City Center Plaza...|          Elgin|       20|\n|        89|     Yoshi|      Tannamuri|        1900 Oak St.|      Vancouver|        5|\n|        90|    Yvonne|        Moncada|Ing. Gustavo Monc...|   Buenos Aires|        1|\n|        91|   Zbyszek|Piestrzeniewicz|     ul. Filtrowa 68|       Warszawa|       14|\n+----------+----------+---------------+--------------------+---------------+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# # Customer Dataframe\n# schema3 = StructType([\n#     StructField(\"CustomerID\", IntegerType(), True),\n#     StructField(\"FirstName\", StringType(), True),\n#     StructField(\"LastName\", StringType(), True),\n#     StructField(\"Address\", StringType(), True),\n#     StructField(\"City\", StringType(), True),\n#     StructField(\"CountryID\", IntegerType(), True)\n# ])\n# mapping_expr = create_map([lit(x) for x in chain(*country_dict.items())]) \n# customer_df=spark.createDataFrame([], schema3)\n# # Splitting Name column into FirstName and LastName\n# from pyspark.sql.functions import split\n\n# name_split = split(df['Name'], ' ',2)\n# df = df.withColumn('FirstName', name_split.getItem(0))\n# df = df.withColumn('LastName', name_split.getItem(1))\n# customer_df = (\n#       df.select('FirstName','LastName','Country','Address','City')\n#       .distinct()\n#       .orderBy('FirstName','LastName')\n#     .withColumn(\"CustomerID\", monotonically_increasing_id() + 1)\n#     .withColumn('CountryID',mapping_expr[col(\"Country\")])\n#     .select(\"CustomerID\",\"FirstName\",\"LastName\",\"Address\",\"City\",\"CountryID\")\n      \n# )\n# customer_df.show(100)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customer_dict = customer_df.rdd.map(lambda row: (row['FirstName'] + ' ' + row['LastName'], row['CustomerID'])).collectAsMap()\ncustomer_dict",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'Alejandra Camino': 1, 'Alexander Feuer': 2, 'Ana Trujillo': 3, 'Anabela Domingues': 4, 'Andre Fonseca': 5, 'Ann Devon': 6, 'Annette Roulet': 7, 'Antonio Moreno': 8, 'Aria Cruz': 9, 'Art Braunschweiger': 10, 'Bernardo Batista': 11, 'Carine Schmitt': 12, 'Carlos Gonzalez': 13, 'Carlos Hernandez': 14, 'Catherine Dewey': 15, 'Christina Berglund': 16, 'Daniel Tonini': 17, 'Diego Roel': 18, 'Dominique Perrier': 19, 'Eduardo Saavedra': 20, 'Elizabeth Brown': 21, 'Elizabeth Lincoln': 22, 'Felipe Izquierdo': 23, 'Fran Wilson': 24, 'Francisco Chang': 25, 'Frederique Citeaux': 26, 'Georg Pipps': 27, 'Giovanni Rovelli': 28, 'Guillermo Fernandez': 29, 'Hanna Moos': 30, 'Hari Kumar': 31, 'Helen Bennett': 32, 'Helvetius Nagy': 33, 'Henriette Pfalzheim': 34, 'Horst Kloss': 35, 'Howard Snyder': 36, 'Isabel de Castro': 37, 'Jaime Yorres': 38, 'Janete Limeira': 39, 'Janine Labrune': 40, 'Jean Fresniere': 41, 'John Steel': 42, 'Jonas Bergulfsen': 43, 'Jose Pavarotti': 44, 'Jose Pedro Freyre': 45, 'Jytte Petersen': 46, 'Karin Josephs': 47, 'Karl Jablonski': 48, 'Laurence Lebihan': 49, 'Lino Rodriguez': 50, 'Liu Wong': 51, 'Liz Nixon': 52, 'Lucia Carvalho': 53, 'Manuel Pereira': 54, 'Maria Anders': 55, 'Maria Larsson': 56, 'Marie Bertrand': 57, 'Mario Pontes': 58, 'Martin Sommer': 59, 'Martine Rance': 60, 'Mary Saveley': 61, 'Matti Karttunen': 62, 'Maurizio Moroni': 63, 'Michael Holz': 64, 'Miguel Angel Paolino': 65, 'Palle Ibsen': 66, 'Paolo Accorti': 67, 'Pascale Cartrain': 68, 'Patricia McKenna': 69, 'Patricio Simpson': 70, 'Paul Henriot': 71, 'Paula Parente': 72, 'Paula Wilson': 73, 'Pedro Afonso': 74, 'Peter Franken': 75, 'Philip Cramer': 76, 'Pirkko Koskitalo': 77, 'Renate Messner': 78, 'Rene Phillips': 79, 'Rita Muller': 80, 'Roland Mendel': 81, 'Sergio Gutierrez': 82, 'Simon Crowther': 83, 'Sven Ottlieb': 84, 'Thomas Hardy': 85, 'Victoria Ashworth': 86, 'Yang Wang': 87, 'Yoshi Latimer': 88, 'Yoshi Tannamuri': 89, 'Yvonne Moncada': 90, 'Zbyszek Piestrzeniewicz': 91}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ProductCategory_rdd = (rdd_df.flatMap(lambda line: [(x, y) for x, y in zip(line[6].split(';'), line[7].split(';'))]) \\\n           .distinct() \\\n           .sortBy(lambda x: (x[0], x[1])) \\\n           .zipWithIndex() \\\n           .map(lambda x: (int(x[1] + 1), x[0][0], x[0][1])) \\\n)\n\n# Convert RDD to DataFrame\nproductcategory_df = ProductCategory_rdd.toDF([\"ProductCategoryID\", \"ProductCategory\", \"ProductCategoryDescription\"])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "productcategory_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+---------------+--------------------------+\n|ProductCategoryID|ProductCategory|ProductCategoryDescription|\n+-----------------+---------------+--------------------------+\n|                1|      Beverages|      Soft drinks, coff...|\n|                2|     Condiments|      Sweet and savory ...|\n|                3|    Confections|      Desserts, candies...|\n|                4| Dairy Products|                   Cheeses|\n|                5| Grains/Cereals|      Breads, crackers,...|\n|                6|   Meat/Poultry|            Prepared meats|\n|                7|        Produce|      Dried fruit and b...|\n|                8|        Seafood|          Seaweed and fish|\n+-----------------+---------------+--------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "pc_dict = productcategory_df.select('ProductCategory', 'ProductCategoryID').rdd.collectAsMap()\npc_dict",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'Beverages': 1, 'Condiments': 2, 'Confections': 3, 'Dairy Products': 4, 'Grains/Cereals': 5, 'Meat/Poultry': 6, 'Produce': 7, 'Seafood': 8}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Product Dataframe\nProduct_rdd=(\nrdd_df.flatMap(lambda line:[(x,y,z) for x,y,z in zip(line[5].split(';'),line[8].split(';'),line[6].split(';'))])\\\n.distinct()\\\n.sortBy(lambda x: x[0])\\\n.zipWithIndex()\\\n.map(lambda x:(int(x[1]+1),x[0][0],float(x[0][1]),int(pc_dict.get(x[0][2],None))))\n)\nproduct_df = Product_rdd.toDF([\"ProductID\",\"ProductName\", \"ProductUnitPrice\", \"ProductCategoryID\"])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "product_df.show(100)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+--------------------+----------------+-----------------+\n|ProductID|         ProductName|ProductUnitPrice|ProductCategoryID|\n+---------+--------------------+----------------+-----------------+\n|        1|        Alice Mutton|            39.0|                6|\n|        2|       Aniseed Syrup|            10.0|                2|\n|        3|    Boston Crab Meat|            18.4|                8|\n|        4|   Camembert Pierrot|            34.0|                4|\n|        5|    Carnarvon Tigers|            62.5|                8|\n|        6|                Chai|            18.0|                1|\n|        7|               Chang|            19.0|                1|\n|        8|    Chartreuse verte|            18.0|                1|\n|        9|Chef Anton's Caju...|            22.0|                2|\n|       10|Chef Anton's Gumb...|           21.35|                2|\n|       11|           Chocolade|           12.75|                3|\n|       12|       Cote de Blaye|           263.5|                1|\n|       13|Escargots de Bour...|           13.25|                8|\n|       14|            Filo Mix|             7.0|                5|\n|       15|         Flotemysost|            21.5|                4|\n|       16|             Geitost|             2.5|                4|\n|       17|        Genen Shouyu|            15.5|                2|\n|       18|Gnocchi di nonna ...|            38.0|                5|\n|       19|   Gorgonzola Telino|            12.5|                4|\n|       20|Grandma's Boysenb...|            25.0|                2|\n|       21|          Gravad lax|            26.0|                8|\n|       22|  Guarana Fantastica|             4.5|                1|\n|       23|    Gudbrandsdalsost|            36.0|                4|\n|       24|        Gula Malacca|           19.45|                2|\n|       25| Gumbar Gummibarchen|           31.23|                3|\n|       26| Gustaf's Knackebrod|            21.0|                5|\n|       27|               Ikura|            31.0|                8|\n|       28|         Inlagd Sill|            19.0|                8|\n|       29|         Ipoh Coffee|            46.0|                1|\n|       30|Jack's New Englan...|            9.65|                8|\n|       31|               Konbu|             6.0|                8|\n|       32|        Lakkalikoori|            18.0|                1|\n|       33|Laughing Lumberja...|            14.0|                1|\n|       34|       Longlife Tofu|            10.0|                7|\n|       35|Louisiana Fiery H...|           21.05|                2|\n|       36|Louisiana Hot Spi...|            17.0|                2|\n|       37|Manjimup Dried Ap...|            53.0|                7|\n|       38|  Mascarpone Fabioli|            32.0|                4|\n|       39|            Maxilaku|            20.0|                3|\n|       40|     Mishi Kobe Niku|            97.0|                6|\n|       41|Mozzarella di Gio...|            34.8|                4|\n|       42|Nord-Ost Matjeshe...|           25.89|                8|\n|       43|Northwoods Cranbe...|            40.0|                2|\n|       44|NuNuCa Nu-Nougat-...|            14.0|                3|\n|       45|Original Frankfur...|            13.0|                2|\n|       46|       Outback Lager|            15.0|                1|\n|       47|        Pate chinois|            24.0|                6|\n|       48|             Pavlova|           17.45|                3|\n|       49|       Perth Pasties|            32.8|                6|\n|       50|      Queso Cabrales|            21.0|                4|\n|       51|Queso Manchego La...|            38.0|                4|\n|       52|Raclette Courdavault|            55.0|                4|\n|       53|      Ravioli Angelo|            19.5|                5|\n|       54|Rhonbrau Klosterbier|            7.75|                1|\n|       55|          Rod Kaviar|            15.0|                8|\n|       56|         Rogede sild|             9.5|                8|\n|       57|   Rossle Sauerkraut|            45.6|                7|\n|       58|       Sasquatch Ale|            14.0|                1|\n|       59|  Schoggi Schokolade|            43.9|                3|\n|       60| Scottish Longbreads|            12.5|                3|\n|       61|Singaporean Hokki...|            14.0|                5|\n|       62|Sir Rodney's Marm...|            81.0|                3|\n|       63| Sir Rodney's Scones|            10.0|                3|\n|       64|      Sirop d'erable|            28.5|                2|\n|       65|           Spegesild|            12.0|                8|\n|       66|      Steeleye Stout|            18.0|                1|\n|       67|      Tarte au sucre|            49.3|                3|\n|       68|Teatime Chocolate...|             9.2|                3|\n|       69|Thuringer Rostbra...|          123.79|                6|\n|       70|                Tofu|           23.25|                7|\n|       71|           Tourtiere|            7.45|                6|\n|       72|            Tunnbrod|             9.0|                5|\n|       73|Uncle Bob's Organ...|            30.0|                7|\n|       74|    Valkoinen suklaa|           16.25|                3|\n|       75|        Vegie-spread|            43.9|                2|\n|       76|Wimmers gute Semm...|           33.25|                5|\n|       77|       Zaanse koeken|             9.5|                3|\n+---------+--------------------+----------------+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "p_dict = product_df.select('ProductName', 'ProductID').rdd.collectAsMap()\np_dict",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'Alice Mutton': 1, 'Aniseed Syrup': 2, 'Boston Crab Meat': 3, 'Camembert Pierrot': 4, 'Carnarvon Tigers': 5, 'Chai': 6, 'Chang': 7, 'Chartreuse verte': 8, \"Chef Anton's Cajun Seasoning\": 9, \"Chef Anton's Gumbo Mix\": 10, 'Chocolade': 11, 'Cote de Blaye': 12, 'Escargots de Bourgogne': 13, 'Filo Mix': 14, 'Flotemysost': 15, 'Geitost': 16, 'Genen Shouyu': 17, 'Gnocchi di nonna Alice': 18, 'Gorgonzola Telino': 19, \"Grandma's Boysenberry Spread\": 20, 'Gravad lax': 21, 'Guarana Fantastica': 22, 'Gudbrandsdalsost': 23, 'Gula Malacca': 24, 'Gumbar Gummibarchen': 25, \"Gustaf's Knackebrod\": 26, 'Ikura': 27, 'Inlagd Sill': 28, 'Ipoh Coffee': 29, \"Jack's New England Clam Chowder\": 30, 'Konbu': 31, 'Lakkalikoori': 32, 'Laughing Lumberjack Lager': 33, 'Longlife Tofu': 34, 'Louisiana Fiery Hot Pepper Sauce': 35, 'Louisiana Hot Spiced Okra': 36, 'Manjimup Dried Apples': 37, 'Mascarpone Fabioli': 38, 'Maxilaku': 39, 'Mishi Kobe Niku': 40, 'Mozzarella di Giovanni': 41, 'Nord-Ost Matjeshering': 42, 'Northwoods Cranberry Sauce': 43, 'NuNuCa Nu-Nougat-Creme': 44, 'Original Frankfurter grune Soe': 45, 'Outback Lager': 46, 'Pate chinois': 47, 'Pavlova': 48, 'Perth Pasties': 49, 'Queso Cabrales': 50, 'Queso Manchego La Pastora': 51, 'Raclette Courdavault': 52, 'Ravioli Angelo': 53, 'Rhonbrau Klosterbier': 54, 'Rod Kaviar': 55, 'Rogede sild': 56, 'Rossle Sauerkraut': 57, 'Sasquatch Ale': 58, 'Schoggi Schokolade': 59, 'Scottish Longbreads': 60, 'Singaporean Hokkien Fried Mee': 61, \"Sir Rodney's Marmalade\": 62, \"Sir Rodney's Scones\": 63, \"Sirop d'erable\": 64, 'Spegesild': 65, 'Steeleye Stout': 66, 'Tarte au sucre': 67, 'Teatime Chocolate Biscuits': 68, 'Thuringer Rostbratwurst': 69, 'Tofu': 70, 'Tourtiere': 71, 'Tunnbrod': 72, \"Uncle Bob's Organic Dried Pears\": 73, 'Valkoinen suklaa': 74, 'Vegie-spread': 75, 'Wimmers gute Semmelknodel': 76, 'Zaanse koeken': 77}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import datetime\n\norders_rdd=(\nrdd_df.flatMap(lambda line:[(line[0],x,y,z) for x,y,z in zip([p_dict.get(i) for i in line[5].split(';')],[datetime.datetime.strptime(j, \"%Y%m%d\").strftime(\"%Y-%m-%d\") for j in line[10].split(';')],line[9].split(';'))])\\\n    .sortBy(lambda x: x[0])\\\n    .zipWithIndex()\\\n    .map(lambda x:(x[1]+1,customer_dict.get(x[0][0]),x[0][1],x[0][2],x[0][3]))\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "orderdetail_df = orders_rdd.toDF([\"OrderID\",\"CustomerID\",\"ProductID\", \"OrderDate\", \"QuantityOrdered\"])\norderdetail_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+----------+---------+----------+---------------+\n|OrderID|CustomerID|ProductID| OrderDate|QuantityOrdered|\n+-------+----------+---------+----------+---------------+\n|      1|         1|       68|2012-08-14|              1|\n|      2|         1|       22|2012-08-14|              6|\n|      3|         1|       66|2012-08-14|              4|\n|      4|         1|       42|2012-08-15|              6|\n|      5|         1|       53|2012-08-15|              2|\n|      6|         1|       42|2012-09-16|             10|\n|      7|         1|       49|2012-09-16|             10|\n|      8|         1|       71|2012-09-16|              5|\n|      9|         1|       42|2014-03-02|              1|\n|     10|         1|        4|2014-03-02|             10|\n|     11|         1|       72|2014-04-09|             10|\n|     12|         1|       61|2014-04-09|              4|\n|     13|         1|       56|2014-04-09|             20|\n|     14|         1|       60|2014-04-09|              2|\n|     15|         1|       15|2014-11-07|             49|\n|     16|         1|       36|2014-11-07|             28|\n|     17|         1|       11|2014-11-07|             33|\n|     18|         1|        5|2014-11-07|              4|\n|     19|         1|       43|2014-11-07|             20|\n|     20|         1|       21|2014-11-07|              1|\n+-------+----------+---------+----------+---------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "orderdetail_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "621806\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dfs=[region_df,country_df,customer_df,productcategory_df,product_df,orderdetail_df]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfs[0].show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+---------------+\n|RegionID|         Region|\n+--------+---------------+\n|       1|  British Isles|\n|       2|Central America|\n|       3| Eastern Europe|\n|       4|  North America|\n|       5|Northern Europe|\n|       6|    Scandinavia|\n|       7|  South America|\n|       8|Southern Europe|\n|       9| Western Europe|\n+--------+---------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dfs[0].columns[0].rstrip(\"ID\").lower()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "'region'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import os\nfrom awsglue.dynamicframe import DynamicFrame\nfor i in dfs:\n    table_name = \"{}_table\".format(i.columns[0].rstrip(\"ID\").lower())\n    file_path = os.path.join(\"s3://sales-data-parquet-ss/sales_db_parquet/\", \"{}.parquet\".format(table_name))\n    i=DynamicFrame.fromDF(i, glueContext, \"convert\")\n    # Assuming glueContext is properly initialized\n    s3output = glueContext.write_dynamic_frame.from_options(\n        frame=i,\n        connection_type=\"s3\",\n        connection_options={\"path\": file_path},\n        format=\"glueparquet\",\n        transformation_ctx=\"s3output\"\n    )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import os\nfor i in dfs:\n    table_name = \"{}_table\".format(i.columns[0].rstrip(\"ID\").lower())\n    file_path = os.path.join(\"s3://sales-data-parquet-ss/sales_db_parquet/\", \"{}.parquet\".format(table_name))\n    i=DynamicFrame.fromDF(i, glueContext, \"convert\")\n    s3output = glueContext.getSink(\n      path=file_path,\n      connection_type=\"s3\",\n      updateBehavior=\"UPDATE_IN_DATABASE\",\n      partitionKeys=[],\n      compression=\"snappy\",\n      enableUpdateCatalog=True,\n      transformation_ctx=\"s3output\",\n    )\n    s3output.setCatalogInfo(\n      catalogDatabase=\"sales_parquet_db\", catalogTableName=table_name\n    )\n    s3output.setFormat(\"glueparquet\")\n    s3output.writeFrame(i)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fbbcb025270>\n<awsglue.dynamicframe.DynamicFrame object at 0x7fbbcb025390>\n<awsglue.dynamicframe.DynamicFrame object at 0x7fbbcb025570>\n<awsglue.dynamicframe.DynamicFrame object at 0x7fbbdb89fe20>\n<awsglue.dynamicframe.DynamicFrame object at 0x7fbbcb0401c0>\n<awsglue.dynamicframe.DynamicFrame object at 0x7fbbcb040f70>\n",
					"output_type": "stream"
				}
			]
		}
	]
}